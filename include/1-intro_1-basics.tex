	
	1959: Feynman's visionary talk, \cite{feynman}; pros: extreme paralelism, cons: reliability.
	
	The ground-breaking work was carried out by Adleman, \cite{adleman94}, who showed that DNA computation is practically feasible. In his experiment, Adleman used special DNA sequences for solving Hamiltonian Path Problem, one of the most typical $\NP$-complete problems.
	
	... Extreme parallelism! But also possibility of errors.

\section*{Work overview}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Chapter 1: Intro.\\
	Chapter 2: First of all we will describe models which exploit specific DNA structure.\\
	Chapter 3: Abstract Tile Assembly Model, temperature, 2D vs. 3D.\\
	Positive integers $\N$. \nomenclature{$\N$}{Positive integers.}
	
	% zkusíme Ramseyovo číslo R(5,5) ? :)
	% -> problém "Je R(n,n) větší než něco?" znamená pro odpověď ANO najít graf t.ž. neobsahuje K_n ani anti-K_n .. což je samo o sobě co-NP (neboli to je \exists \forall stroj .. \Sigma_2 jazyk, horní hranice pak je \Pi_2 jazyk)
	% 
	% NP vs co-NP .. co-NP přece nedává ne?

\section{Basic DNA principles}
	
	DNA (deoxyribonucleic acid) is a large biomolecule carrying living organisms' genetic information. Its most common structure is well known double-helix which consists of two strands connected by hydrogen bonds. These strands are biopolymers built up by {\em polymerase chain reaction} (PCR) from small units -- nucleotides. Each nucleotide consists of two parts: nitrogenous base and backbone molecules.
	\begin{description}
		\item[Nitrogenous bases] There are $4$ nucleobases in DNA ($+1$ in RNA): Adenine ({\bf A}), Thymine ({\bf T}), Cytosine ({\bf C}), Guanine ({\bf G}) and Uracil ({\bf U}) in RNA instead of Thymine in DNA. These molecules are responsible for making hydrogen bonds between strands in a manner following Watson-Crick complementarity: only {\bf A} -- {\bf T} and {\bf C} -- {\bf G} pairs can be formed.
		\item[Backbone molecules] Backbone of DNA strand is made of alternating deoxyriboses and phosphates. Phosphates only hold adjacent deoxyriboses, each deoxyribose moreover holds one nucleobase. Due to deoxyribose carbon numbering, DNA backbone has so called $5'$ and $3'$ ends, default reading order is $5'\rightarrow 3'$. DNA strands must be antiparallel so that nucleobases can connect.
	\end{description}

\section{Complexity, languages}
	
	\subsection{$O$ and other notations}
		
		Let us briefly remind $O$-, $o$-, $\Omega$-, $\omega$- and $\Theta$-notations for $f, g: \N \rightarrow \N$. We will denote $(\exists n_0 \in \N) (\forall n > n_0)$ shortly by $(\forall^* n)$ which can be read ``for almost all $n$''. Also $(\forall n_0 \in \N) (\exists n > n_0)$ will be denoted by $(\exists^\infty n)$ which can be read ``there exist infinitedly many $n$''.
		\begin{defn}
		\begin{align*}
			g \in O(f) &\iff (\exists C > 0) (\forall^* n) \bigl(g(n) < C f(n)\bigr) \\[5pt]
			g \in \omega^{(1)}(f) &\iff (\forall C > 0) (\exists^\infty n) \bigl(g(n) > C f(n)\bigr) \\[5pt]
			g \in \omega^{(2)}(f) &\iff (\forall C > 0) (\forall^* n) \bigl(g(n) > C f(n)\bigr) \\[5pt]
			g \in \Omega^{(1)}(f) &\iff (\exists C > 0) (\exists^\infty n) \bigl(g(n) > C f(n)\bigr) \\[5pt]
			g \in \Omega^{(2)}(f) &\iff (\exists C > 0) (\forall^* n) \bigl(g(n) > C f(n)\bigr) \\[5pt]
			g \in o(f) &\iff (\forall C > 0) (\forall^* n) \bigl(g(n) < C f(n)\bigr) \\[5pt]
			g \in \Theta(f) &\iff (\exists C_1, C_2 > 0) (\forall^* n) \bigl(C_1 f(n) \leq g(n) \leq C_2 f(n)\bigr) \\[5pt]
			g \sim f &\iff \lim\limits_{n\to\infty} \frac{g(n)}{f(n)} = 1
		\end{align*}
		\end{defn}
		
		\begin{remark}
			Note that there are two different definitions for omegas. $\Omega^{(1)}$ is equivalent to the orgininal definition introduced by Hardy \cite{hardy1914} which states
			\begin{equation*}
				f \in \Omega(g) \iff \limsup\limits_{n\to\infty}\frac{f(n)}{g(n)} > 0 .
			\end{equation*}
			The other, $\Omega^{(2)}$, was introduced by Knuth from good reasons described in \cite{knuth76}. In similar manner there are two definitions for $\omega$.
			
			Note that there are also some relations: the condition for $\Omega^{(1)}$ is negation of the condition for $o$ so these sets are complementary for given function $f$, similarly for $\omega^{(1)}$ and $O$. For the second variant one can easily check that $f\in\Omega^{(2)}(g)\iff g\in O(f)$ and $f\in\omega^{(2)}(g)\iff g\in o(f)$. There is also an equivalent condition for $\Theta$:
			\begin{equation*}
				g \in \Theta(f) \iff g \in O(f) \,\wedge\, f \in O(g) \iff g \in O(f) \,\wedge\, g \in \Omega^{(2)}(f) .
			\end{equation*}
			
			The condition for $g \sim f$ can be easily seen to be equivalent to $|f-g| \in o(g)$. In chapter~\ref{chap:problems} we will be mostly interested in this relation because it specifies the function better than $\Theta$. For example, $2n \in \Theta(n)$ but $2n \not\sim n$.
		\end{remark}
	
	\subsection{Studied complexities}
		
		\begin{defn}
			All the following complexities are considered as functions of problem size $n$:
			\begin{description}
				\item[Biostep complexity] will refer to the number of laboratory steps required to handle the computation, denoted by $Bs(n)$.
				\item[Binding complexity] will refer to the number of bindings in given computation, denoted by $Bnd(n)$.
				\item[Tile complexity] will refer to the number of different DNA tiles, denoted by $Ti(n)$.
				\item[Glue complexity] will refer to the number of different sticky-end sequences (commonly referred to as {\em glues}), denoted by $Gl(n)$. Each sequence with its Watson-Crick complement is considered as one glue.
			\end{description}
		\end{defn}
		
		\begin{remark}
			Note some properties of proposed complexities.
			\begin{description}
				\item[Ad biostep complexity] Adleman \cite{adleman95biostep} describes formally in his {\em unrestricted model} few types of such lab procedures -- {\em Separate, Merge, Detect} and {\em Amplify}, Winfree \cite{winfree_phd} adds another -- {\em Append}. And both of them remind that one biostep takes tens of minutes. Thus the only practically feasible DNA algorithms are those with $O(1)$ biostep complexity.
				\item[Ad binding complexity] Too high binding complexity leads to lower probability of correct computation $P_c$ because it holds $P_c(n) = (1-p_e)^{Bnd(n)} \approx\footnote{If reasonable.} \; 1-p_e \cdot Bnd(n)$ where $p_e$ denotes probability of erroneous binding.
				\item[Ad tile complexity] The higher tile complexity the more demanding it is to prepare required tiles.
				\item[Ad glue complexity] Higher glue complexity will require longer DNA sequences in the sticky ends. % which leads to hogher probability of erroneous binding $p_e$. %!% něco citovat co zminuje závislost p_e na dýlce sekvence, to přece musí rost ...
			\end{description}
		\end{remark}
	
	\subsection{Languages}
		
		\begin{defn}
			Let $\Sigma$ be an nonempty and finite set of {\em characters} which will be referred to as {\em alphabet}. Define set of {\em words} over alphabet $\Sigma$ as $\Sigma^* = \bigcup_{n\in\N_0}\Sigma^n$ and set of nonempty words as $\Sigma^+ = \bigcup_{n\in\N}\Sigma^n$. Empty word will be denoted by $\varepsilon$. {\em Language} ${\cal L}$ over alphabet $\Sigma$ is then just a set of words: ${\cal L} \subseteq \Sigma^*$. {\em Complement} of language $\cal L$ will be denoted by ${\cal\overline L} = \Sigma^* \setminus {\cal L}$.
		\end{defn}
		
		Let us briefly remind Chomsky hierarchy of languages generated by generative grammars. Denote alphabet of non-terminals by $N$, alphabet of terminals by $\Sigma$ and initial symbol by $I$.
		\begin{description}
			\item[Recursively enumerable (type 0)] Generated by unrestricted grammar rules.
			\item[Context-sensitive (type 1)] Grammar rules are either all of the form $\alpha A \beta \rightarrow \alpha \eta \beta$ where $\alpha,\,\beta\in (N\cup\Sigma)^*$, $A\in N$ and $\eta\in (N\cup\Sigma)^+$. Note that $\eta$ cannot be empty. Or, if we assume these rules without those having $I$ on the right hand side, then there is allowed also the rule $I\rightarrow \varepsilon$.
			\item[Context-free (type 2)] All grammar rules are of the form $A\rightarrow\eta$ where $A\in N$ and $\eta\in (N\cup\Sigma)^*$. Note that $\eta$ can be empty.
			\item[Regular (type 3)] Grammar rules are either all of the forms $A\rightarrow bB$ and $A\rightarrow b$ where $A,\,B\in N$ and $b\in\Sigma$. Or, if we assume these rules without those having $I$ on the right hand side, then there is allowed also the rule $I\rightarrow \varepsilon$.
		\end{description}
		
		\begin{remark}
			Note that given an alphabet $\Sigma$ the set of all words $\Sigma^*$ is countable. Moreover, one can sort $\Sigma^*$ first by word length, then lexicographically thus it is easy to define desired bijection $f: \N \leftrightarrow \Sigma^* $.
			
			Formal language ${\cal L}$ can then be viewed as a boolean function $g: \N \rightarrow \{0,1\}$ defined as
			$g(n) = 1 \iff f(n)\in {\cal L}$.
		\end{remark}
		
		% přidat čim je co rozpoznávatelný?
		% abeceda \Sigma -> {0, 1} ??
	
	\subsection{$\P$, $\NP$ and other}
	\label{sec:PNP}
		
		This section describes few classes of languages from the resource-consumption point of view. There exist many equivalent definitions, these are taken from \cite{book_comp}. % The first set of definitions is very straightforward and thus can be used for propositions.
		
		\begin{defn}\label{def:P}
			${\cal L} \subseteq \Sigma^*$. ${\cal L} \in \P \iff \bigl(\exists p \in {\cal P}\bigr) \bigl(\exists \textnormal{ deterministic Turing machine } M\bigr)\\ \bigl(\forall x \in \Sigma^*\bigr) \bigl(x \in {\cal L} \iff M \textnormal{ accepts } x \textnormal{ in time } \leq p(|x|)\bigr)$.
		\end{defn}
		
		\begin{defn}\label{def:NP}
			${\cal L} \subseteq \Sigma^*$. ${\cal L} \in \NP \iff \bigl(\exists p, q \in {\cal P}\bigr) \bigl(\exists \textnormal{ deterministic Turing machine } M\bigr)\\ \bigl(\forall x \in \Sigma^*\bigr) \Bigl(x \in {\cal L} \iff \bigl(\exists y \in \Sigma^{p(|x|)}\bigr) \bigl(M \textnormal{ accepts } (x,y) \textnormal{ in time } \leq q(|x|+|y|)\bigr)\Bigr)$.\\
			Such $y$ will be referred to as {\em certificate} for $x$ (with respect to $\cal L$ and $M$).
		\end{defn}
		
		\begin{defn}\label{def:coNP}
			${\cal L} \subseteq \Sigma^*$. ${\cal L} \in \coNP \iff \bigl(\exists p, q \in {\cal P}\bigr) \bigl(\exists \textnormal{ deterministic Turing machine } M\bigr)\\ \bigl(\forall x \in \Sigma^*\bigr) \Bigl(x \in {\cal L} \iff \bigl(\forall y \in \Sigma^{p(|x|)}\bigr) \bigl(M \textnormal{ accepts } (x,y) \textnormal{ in time } \leq q(|x|+|y|)\bigr)\Bigr)$.
		\end{defn}
		
		The second possibility is to first define general classes $\DTime$, $\NTime$, $\DSpace$ and $\NSpace$ and after that to use their special case. Remind how Non-deterministic Turing machine (NTM) differs from Deterministic Turing machine (DTM):
		\begin{itemize}
			\item it is allowed to have ambiguous transition function,
			\item it has two types of terminal states: accepting $q_{accept}$ and halting without accepting $q_{halt}$ (not rejecting!). We say it accepts input $x$ iff there {\em exists} a sequence of decisions ending in $q_{accept}$, it declines $x$ iff {\em for every} sequence of decisions reaches $q_{halt}$.
		\end{itemize}
		
		%!% tady je problém s negací v D---- ptž polynom časově konstruovatelnej je ale co když f(n) neni že ... ale asi mi to neva ...
		\begin{defn}\label{def:DTime}
			$\DTime\bigl(f(n)\bigr) = \bigl\{ {\cal L}\textnormal{ language} \bigm| (\exists \textnormal{ deterministic Turing machine }M)\\ (\forall x\in\Sigma^*)\bigl(x\in{\cal L} \iff M\textnormal{ accepts }x\textnormal{ in time }\leq f(|x|)\bigr) \bigr\}$.\\
			Other classes are defined in a very similar manner: time $\leftrightarrow$ space, deterministic $\leftrightarrow$ non-deterministic.
		\end{defn}
		
		\begin{defn}
			$\P = \bigcup\limits_{k\in\N} \DTime(n^k)$ and $\NP = \bigcup\limits_{k\in\N} \NTime(n^k)$.
		\end{defn}
		
		\begin{defn}
			$ {\cal L} \in \coNP \iff {\cal\overline L} \in \NP $.
		\end{defn}
		
		\begin{note}
			$\NP\subseteq\powerset{\Sigma^*}$ so the notation $\coNP$ might be confusing. Note that $\coNP$ is {\em not} a complement to $\NP$ as a set of languages.
		\end{note}
		
		\begin{remark}
			In case of $\P$ there is no ``co'' version. It follows from ${\cal L} \in \P \iff \overline{\cal L} \in \P$. This can be easily seen because deterministic Turing machine is capable of negation with the same time resources, non-deterministic is not known to. Note that Immerman--Szelepcsényi theorem states possibility of non-deterministic Turing machine negation in limited {\em space}: ${\cal L} \in \NSpace(s(n)) \iff \overline{\cal L} \in \NSpace(s(n))$ for $s(n) \geq \log(n)$.   %!% problematickej: \footnote{Since $\P = \NP = \coNP$ is not known.}
		\end{remark}
		
		\begin{thm}
			All the previous definitions of $\P$, $\NP$ and $\coNP$ are equivalent.
		\end{thm}
		
		\begin{defn}
			We say that language ${\cal L}_1$ is {\em polynomial-time Karp reducible} to ${\cal L}_2$ and denote ${\cal L}_1\propto{\cal L}_2 \iff \exists$ polynomial-time computable function $f:\Sigma^*\rightarrow\Sigma^*$ such that $(\forall x\in\Sigma^*)(x\in{\cal L}_1\iff x\in{\cal L}_2)$.
		\end{defn}
		
		\begin{defn}
			${\cal L}\in\NPH \iff (\forall{\cal L'}\in\NP)({\cal L'}\propto{\cal L})$.
		\end{defn}
		
		\begin{defn}
			${\cal L}\in\NPC\iff{\cal L}\in\NP\cap\NPH$.
		\end{defn}
		
		\begin{note}
			If we found a polynomial-time algorithm for {\em any} $\NPC$ language, it would hold $\P=\NP$ which is not believed to be true. Thus supposed that $\P\neq\NP$ there does not exist a polynomial algorithm for any $\NPC$ language.
		\end{note}
		
		\begin{example}\label{exm:npc}
			Remind some popular $\NPC$ problems: boolean formula satisfability ($\mathsf{SAT}$), Hamiltonian path problem ($\mathsf{HPP}$), graph $3$-coloring, graph $k$-independent set, graph $k$-clique, graph $k$-vertex cover, subset sum and many others.
			
			On the other hand there are few interesting problems which are not known to belong either to $\P$ nor to $\NPC$, one of them is graph isomorphism problem.
		\end{example}
		
		% poly-time hierarchy ... book pg. 93
		
		To define probabilistic classes of languages ($\BPP$, $\RP$, $\coRP$ and $\ZPP$) we will remind the concept of Probabilistic Turing machine (PTM). Like NTM it is allowed to have ambiguous transition function, moreover, in every state there is defined a transition probability and it can have another terminal state $q_{reject}$.
		
		We say that PTM $M$
		\begin{itemize}
			\item decides language $\cal L$ iff for every $x\in\Sigma^*$ the probability of halting in correct state (i.e. $q_{accept}$ for $x\in{\cal L}$ and $q_{reject}$ for $x\notin{\cal L}$) is higher than $\nicefrac{2}{3}$.
			\item decides language $\cal L$ in time $t(n)$ iff $M$ decides language $\cal L$ and for every $x\in\Sigma^*$ it halts in time $\leq t(|x|)$.
		\end{itemize}
		Now we can define those classes, note that it would be possible to use again two types of definition like above.
		
		% příp. pg 121 .. tam je alternativní def
		
		\begin{defn}\label{def:BPTime}
			$\BPTime\bigl(f(n)\bigr) = \bigl\{ {\cal L}\textnormal{ language} \bigm| (\exists \textnormal{ probabilistic Turing machine }M)\\ \bigl(M\textnormal{ decides }{\cal L}\textnormal{ in time }\leq f(|x|)\bigr) \bigr\}$.
		\end{defn}
		
		\begin{defn}
			$\BPP = \bigcup\limits_{k\in\N} \BPTime(n^k)$.
		\end{defn}
		
		\begin{defn}
			$\RTime\bigl(f(n)\bigr) = \bigl\{ {\cal L}\textnormal{ language} \bigm| (\exists \textnormal{ probabilistic Turing machine }M)\\ (\forall x\in\Sigma^*)\bigl(M \textnormal{ halts in time } \leq f(|x|) \;\wedge\; x\in{\cal L} \then \prob(M\textnormal{ accepts }x) \geq \nicefrac{2}{3} \;\wedge\\ x\notin{\cal L} \then \prob(M\textnormal{ accepts }x) = 0 \bigr) \bigr\}$.
		\end{defn}
		
		\begin{defn}
			$\RP = \bigcup\limits_{k\in\N} \RTime(n^k)$.
		\end{defn}
		
		\begin{defn}
			$ {\cal L} \in \coRP \iff {\cal\overline L} \in \RP $.
		\end{defn}
		
		\begin{remark}
			$\BPP$ allows PTM to make both wrong decisions (for $x\in\cal L$ as well as for $x\notin\cal L$), on the other hand $\RP$ does not allow error for $x\notin\cal L$ and $\coRP$ does not allow error for $x\in\cal L$. Following class $\ZPP$ (from {\em zero error}) does not allow any error, it only allows halting neither with accepting nor with rejecting.
		\end{remark}
		
		%!% proč mam pocit že to maj v book blbě definovaný? takle by to znamenalo P=ZPP. líp u majerecha!
		
		\begin{defn}
			$\ZPTime\bigl(f(n)\bigr) = \Bigl\{ {\cal L}\textnormal{ language} \Bigm| \bigl(\exists \textnormal{ probabilistic Turing machine }M\bigr)\\ \bigl(\forall x\in\Sigma^*\bigr)\Bigl(M \textnormal{ halts in time } \leq f(|x|) \;\wedge\\ \bigl(x\in{\cal L} \then \prob(M\textnormal{ accepts }x) \geq \nicefrac{2}{3} \,\wedge\, \prob(M\textnormal{ rejects }x) = 0 \bigr) \;\wedge\\ \bigl(x\notin{\cal L} \then \prob(M\textnormal{ rejects }x) \geq \nicefrac{2}{3} \,\wedge\, \prob(M\textnormal{ accepts }x) = 0 \bigl) \Bigr) \Bigr\}$.
		\end{defn}
		
		\begin{defn}
			$\ZPP = \bigcup\limits_{k\in\N} \ZPTime(n^k)$.
		\end{defn}
		
		\begin{thm}
			\begin{equation*}
				{\cal L}_{1,2}\in\ZPP\then{\cal\overline L}_1,\,{\cal L}_1\cap{\cal L}_2,\,{\cal L}_1\cup{\cal L}_2\in\ZPP .
			\end{equation*}
		\end{thm}
		
		\begin{thm}
			\begin{align*}
				\P \subseteq \ZPP &= \RP \cap \coRP , \\
				\RP &\subseteq \NP\cap\BPP , \\
				\coRP &\subseteq \coNP\cap\BPP .
			\end{align*}
		\end{thm}
		
		% PP, \#P, PSpace ..? Maybe also polynomial hierarchy: $\Sigma_k P$ and $\Pi_k P$ languages (alternating Turing machine with bounded alternation, \cite{kozen06}). Might be nice to show what DNA computation is capable of.
		
		% SAT: Lipton's contribution using $m$ biosteps ($m = \#clauses$), \cite{lipton95}, Lipton's set of speedup problems, \cite{lipton96speedup}.
		% Energy efficiency (Adleman).